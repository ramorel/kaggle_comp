---
title: "Comparing R and Python to Predict Housing Prices"
author: "Richard Paquin Morel"
date: "1/22/2020"
output:
  html_document:
    keep_md: true
    number_sections: true
    toc: true
    toc_depth: 2
    theme: yeti
    highlight: zenburn
---

# Introduction

R and Python are both great for statistical programming, but have their relative strengths and weaknesses. I will sum these differences up, poorly, as: R is generally better at inferential statistics, Python is generally better at predictive statistics--and machine learning more specifically. As a summary statement and generalization, that is both helpful and wrong. But gives the general idea.

I thought it would be fun, as an exercise, to do a side-by-side, nose-to-tail analysis in both R and Python, taking advantage of the wonderful [`{reticulate}`](https://rstudio.github.io/reticulate/) package in R. `{reticulate}` allows one to access Python through the R interface. I find this especially cool in Rmarkdown, since you can knit R and Python chucks in the same document! You can, to some extent, pass objects back and forth between the R and Python environments. Wow. 

So given this affordance, I analyze the Ames Housing dataset with the goal of predicting housing prices, a la [this Kaggle competition](https://www.kaggle.com/c/house-prices-advanced-regression-techniques).

I abide by a few rule:

- Be explicit as possible in both languages
- Use a few packages/libraries as possible
- Level playing field--e.g., use the same values for tuning parameters
- Try to replicate the analysis as closely as possible in each language
- Avoid any sort of evaluative comparison

_Important note._ I primarily use R and have much more experience with R than Python. So bear that in mind.

Let the fun begin.

![](/Users/richardmorel/Documents/GitHub/good_time_fun_time/files/it_begins.gif)

# Importing the libraries

Now, I know I just said that I will use as few libraries as possible--but I will do the R end of this in the `{tidyverse}` idiom. I will also take a few things from the `{tidymodels}` suite of packages. I do this because `{tidymodels}` is the successor to one of the main machine learning packages in R, `{caret}`.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse)
library(tidymodels)
library(patchwork)
library(glue)

# Define theme for ggplot
theme_set(
  theme_minimal() +
    theme(
      axis.title.x = element_text(size = 12, hjust = 1),
      axis.title.y = element_text(size = 12),
      axis.text.x = element_text(size = 10),
      axis.text.y = element_text(size = 10),
      plot.title = element_text(size = 12, colour = "grey25", face = "bold"),
      plot.subtitle = element_text(size = 11, colour = "grey45"))
  )

# Get ready for python
library(reticulate)
```

For Python, I use the usual suspects: `{pandas}`, `{numpy}` and `{scipy}`, and `{matplotlib}` for visualizations. I also import `{seaborn}`, again violating my rules, but `{seaborn}` makes it easy to color visualizations by groups, like in `{ggplot2}`.
```{python}
import pandas as pd
import numpy as np
from scipy.stats import skew
import seaborn as sns
import matplotlib.pyplot as plt
```

## Importing the data

The cool thing about `{reticulate}` is that it allows you to access objects from either environment. To access a Python object, you use the syntax `r.object` and in R you use `py$object`. So in this case, I will import using Python and then call the objects over to the R environment. 
This causes an interesting data issue and serves as a good reminder to always check out your data before plunging ahead with analysis. 
```{python}
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

# Keep the IDs from the test data for submission
ids = test['Id']
```

Using the syntax just described, I will assign the objects in R.
```{r}
train <- py$train
test <- py$test

# Keep IDs for submission
ids <- test %>% pull(Id)
test <- test %>% select(-Id)

# Keep outcome variable
y <- train$SalePrice
```

# Exploratory data analysis

## Examining the structure of the data

First, it is always a good idea to know exactly _what_ the data look like. Key questions:

1. How many variables/features/columns do I have?
2. How many observations/rows do I have?
3. What types of variables/features/columns do I have?
4. Which variables/features/columns have missing observations and how many?

Let's do this in R first. Check the dimensions and then the features. 
```{r}
print(glue("The training dataset has {ncol(train)} features and {nrow(train)} observations."))

map_chr(train, class) %>% table() %>% {glue("There are {.} features of type {names(.)}")}
```

I like the `glimpse()` function from the `{tibble}` package a lot--it gives the dimension, the features, their type, and a quick glance at each. But it isn't great where there are a great deal of features. So, check the dimensions first. Here, it's not bad at all, so I use `glimpse()` to see the features and their type.
```{r}
glimpse(train)
```

Before moving onto Python, notice that some of the features are of class `list`. This is an artifact of importing via Python and then calling into the R environment. So we have some cleaning up to do. No problem. First, we will `unlist()` those features and then make sure that the missing values are encoded correctly, since, as you will see, the missing values are encoded now as strings!
```{r}
train <- train %>% mutate_if(is.list, unlist)

train <- train %>% na_if("NaN")

test <- test %>% mutate_if(is.list, unlist)

test <- test %>% na_if("NaN")

any(map_lgl(train, is.list))
```

That looks much better.

Now, I will do the same thing in Python. _Quick reminder that Python is zero-indexed!!_
```{python}
print("The training dataset has %s features and %s observations" % (train.shape[1], train.shape[0]))

#for col in train.columns:
#  print("Feature {} is of type {}".format(col, train[[col]].dtypes[0]))
  
for name, val in zip(list(train.dtypes.value_counts().index), train.dtypes.value_counts()):
  print('There are {} features of type {}'.format(val, name))

```

The method `head()` in `{pandas}` does something similar to `glimpse()`. It gives the number of features along with the first _n_ observation (by default 5, but I will use 10). Unlike `glimpse()` it does not give the type of each feature. But that's okay, we just got that information.
```{python}
train.head(10)
```

So we know there are 81 features and 1,460 observations. Very important for getting aquainted with a new dataset is to understand the missing data--which features have missing observations and how many. Just as important is to know (or figure out) the process by which missing values are generated. Is it due to random or systematic? Is it due to non-compliance/non-reporting? Data entry mistakes? Missing due to specific suppression rules? In the case of this data, the missing values are _meaningful_. Missing often--but not always--means "This house does not have this feature." Right now, we will just describe the missing data.

First, in R. There are many ways to approach this. A simple way is `train %>% summarise_all(~sum(is.na(.)))`, but the output is not reader-friendly. So I'll pretty it up a bit. Note: wrapping the `map2` function in `{}` prevents the piped data frame from being used as the first argument--we need that in order to call `.[["name"]]` and `.[["value"]]` as the two variables to loop over.
```{r}
map_dbl(train, ~ sum(is.na(.x))) %>% table() %>% {glue("There {ifelse(. == 1, 'is', 'are')} {.} {ifelse(. == 1, 'feature', 'features')} with {names(.)} missing values")}
```


```{r}
train %>% 
  summarise_all(~sum(is.na(.))) %>% 
  pivot_longer(cols = everything()) %>% 
  filter(value != 0) %>% 
  arrange(desc(value)) %>% 
  {glue("{.[['name']]} has {.[['value']]} missing observations")}
```

We can also do this by percent, which provides an additional layer of information.
```{r}
train %>% 
  summarise_all(~round(sum(is.na(.))/length(.), 4) * 100) %>% 
  pivot_longer(cols = everything()) %>% 
  filter(value != 0) %>% 
  arrange(desc(value)) %>% 
  {glue("{.[['name']]} has {.[['value']]}% missing observations")}
```

Now to do this in Python. `{pandas}` has a nifty `is.null()` method that we an chain with `sum()` to get the number of missing values in a series.
```{python}
mv = train.isnull().sum()
mv = mv[mv!=0].sort_values(ascending=False)

for name, val in zip(list(mv.index), mv):
  print("{} has {} missing values".format(name, val))
```

And dividing by the length of the series gets us the proportion. 
```{python}
mv = (train.isnull().sum()/len(train))*100
mv = mv[mv!=0].sort_values(ascending=False).round(2)

for name, val in zip(list(mv.index), mv):
  print("{} has {}% missing values".format(name, val))

#for col in train.columns:
#  prop = (train[col].isnull().sum()/len(train[col])) * 100
#  print("{} has {}% missing values".format(col, prop.round(2)))
```

## Exploring the target feature/dependent variable/outcome
Attending to the characteristics of the target variable is a critical part of data exploration. It helps to visualize the distribution to determine the type of analysis you might want to carry out.

Knowing to content domain, we would not be surprised to see that the distribution is right-skewed, as things like housing prices, income, etc. often are. 
```{r}
p1 <- train %>%
  ggplot(aes(x = SalePrice)) +
  geom_histogram(aes(x = SalePrice, stat(density)),
                 bins = 100,
                 fill = "cornflowerblue",
                 alpha = 0.7) +
  geom_density(color = "midnightblue") +
  scale_x_continuous(breaks= seq(0, 800000, by=100000), labels = scales::comma) +
  labs(x = "Sale Price", y = "", title = "Density of Sale Price") +
  theme(axis.text.x = element_text(size = 10, angle = 45, hjust = 1))

p1
```

In such cases, it is useful to take the log of the feature in order to normalize its distribution. That way, it satisfies the expectations of linear modeling. 

```{r}
p2 <- train %>%
  ggplot() +
  geom_histogram(aes(x = log(SalePrice), stat(density)),
                 bins = 100,
                 fill = "cornflowerblue",
                 alpha = 0.7) +
  geom_density(aes(x = log(SalePrice)), color = "midnightblue") +
  labs(x = "Natural log of Sale Price", y = "", title = "Density of Logged Sale Price")

p1 + p2
```

Again, in Python, looking at the plots side by side.
```{python}
y = train['SalePrice']

fig, ax = plt.subplots(1, 2, sharey=False, sharex=False, figsize=(10,6))
sns.distplot(y, ax=ax[0]).set(title='Density of Sale Price')
sns.distplot(np.log(y), ax=ax[1]).set(title='Density of Logged Sale Price')
```

That looks much better. Next, let's explore the features.

## Exploring the features


## Exploring some bivariate relationships

We might guess, based on our prior knowledge, that there is a meaningful positive relationship between how much a house costs and how big it is.

```{r}
p1 <- train %>%
  ggplot(aes(x = GrLivArea, y = SalePrice)) +
  geom_point(color = "cornflowerblue", alpha = 0.75) +
  scale_y_continuous(breaks= seq(0, 800000, by=200000), labels = scales::comma) +
  labs(x = "Above ground living area", y = "Sale Price", title = "Sale Price \nby Above Ground Living Area")

p2 <- train %>%
  ggplot(aes(x = GrLivArea, y = log10(SalePrice))) +
  geom_point(color = "cornflowerblue", alpha = 0.75) +
  labs(x = "Above ground living area", y = "Log of Sale Price", title = "Logged Sale Price \nby Above Ground Living Area")

p1 + p2  
```

In Python, we can use `{matplotlib}` to see this relationship.
```{python}
fig, ax = plt.subplots(1, 2, sharey=False, sharex=False, figsize=(10,6))
ax[0].scatter(train['GrLivArea'], train['SalePrice'], alpha = 0.75, color = '#6495ED')
ax[0].set_title('Sale Price \nby Above Ground Living Area')
ax[1].scatter(train['GrLivArea'], np.log(train['SalePrice']), alpha = 0.75, color = '#6495ED')
ax[1].set_title('Logged Sale Price \nby Above Ground Living Area')
plt.show()
```

In both plots, we can see a few outliers--partularly the houses with relative low sale prices given their above ground living area. It would be wise to drop these, since there are probably reasons that they are unusual and our features won't pick that up.

But, out of curiosity, let's see if there is some reason _in the data_ why these house sold for so little money. Perhaps it has to do with the overall quality of the house? Its type? Its zoning? Its location?
```{r}
p1 <- train %>%
  ggplot(aes(x = GrLivArea, y = SalePrice)) +
  geom_point(aes(color = factor(OverallQual)), alpha = 0.75) +
  scale_color_viridis_d(name = "Overall Quality", 
                        breaks = c("2", "4", "6", "8", "10")) +
  scale_y_continuous(breaks= seq(0, 800000, by=200000), labels = scales::comma) +
  labs(x = "Above ground living area", y = "Sale Price", title = "Sale Price \nby Above Ground Living Area") +
  theme(legend.position = c(0.2, 0.8),
        legend.title = element_text(size = 8),
        legend.text = element_text(size = 6),
        legend.background = element_blank(),
        legend.key.size = unit(5, "mm"),
        legend.box.background = element_rect(color = "black", fill = "transparent"),
        legend.margin = ggplot2::margin(0.1, 0.1, 0.1, 0.1, "cm"))

p2 <- 
  train %>%
  mutate(MSZoning = factor(MSZoning, levels = c("RL", "RM", "C (all)", "FV", "RH"))) %>% 
  ggplot(aes(x = GrLivArea, y = SalePrice)) +
  geom_point(aes(color = MSZoning), alpha = 0.75) +
  scale_color_viridis_d(name = "Zoning", end = 0.7) +
  scale_y_continuous(breaks= seq(0, 800000, by=200000), labels = scales::comma) +
  labs(x = "Above ground living area", y = "Sale Price", title = "Sale Price \nby Above Ground Living Area") +
  theme(legend.position = c(0.2, 0.8),
        legend.title = element_text(size = 8),
        legend.text = element_text(size = 6),
        legend.background = element_blank(),
        legend.key.size = unit(5, "mm"),
        legend.box.background = element_rect(color = "black", fill = "transparent"),
        legend.margin = ggplot2::margin(0.1, 0.1, 0.1, 0.1, "cm"))

p1 + p2
```

For my Python version, I will use `{seaborn}` rather than straight `{matplotlib}` to easily color the points by category.
```{python}
fig, ax = plt.subplots(1, 2, sharey=False, sharex=False, figsize=(10,6))
sns.scatterplot('GrLivArea', 'SalePrice', hue='OverallQual', data=train, palette='viridis', ax=ax[0], alpha=0.75)
ax[0].set_title('Sale Price \nby Above Ground Living Area')
ax[0].set_ylabel('Sale Price')
sns.scatterplot('GrLivArea', 'SalePrice', hue='MSZoning', data=train, palette='viridis', ax=ax[1], alpha=0.75)
#ax[1].set(title='Sale Price \nby Above Ground Living Area', xlabel='Above ground living area', ylabel = 'Sale Price')
fig.tight_layout(pad=3.0)
plt.show()
```

We should drop those outliers. 
```{r}
train <-
  train %>%
  filter(
    !(GrLivArea > 4000 & SalePrice < 200000)
  )
```

```{python}
train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index, inplace = True)
```

# Dealing with missing values

We saw before that there are many features without missing values, but a bunch that have a lot. For some of those features, the missing values are meaningful--missing indicates that the house lacks that feature. Take, for example, pools. The feature `PoolQC`, indicate pool quality, has `r sum(is.na(train$PoolQC))` missing observations. They are missing because a pool that doesn't exist can't be assigned a quality! For features like this, it might make sense to assign a value like `None` if it is categorical and `0` if it is numeric. There are sets of related features--for example, those related to basements. If a home does not have a basement, clearly it will not have quality, condition, square footage, etc. related to that basement. So, for the categorical values, I will replace missing with `None` and for the numeric, I will replace missing with `0`.

There are a handful of other features were is seems like we have true missing values--missing for some unknown reason, but certainly present in reality. For example, the `KitchenQual` feature. Certainly, all the homes in this dataset have kitchens, as I believe that is a legal requirement! So, this value is likely missing at random. There are a variety of imputation strategies. I will impute by the mode of the neighborhood and subclass, on the reasoning that homes within neighborhoods and classes are often similar. We might also consider a more sophisticated approach using regression or classification approaches.

To do this, we want to combine the training and testing data so we can fixing the missing values and do the feature engineering all at once. And any scaling or centering we do, we want to do on the all the data at our disposal. We should also drop the target and the ID feature.

```{r}
all_dat <- bind_rows(train, test) %>% 
  select(-Id, -SalePrice)

none_vars <- c("BsmtCond",
               "BsmtQual",
               "BsmtExposure",
               "BsmtFinType1",
               "BsmtFinType2",
               "MasVnrType",
               "GarageType",
               "GarageFinish",
               "GarageQual",
               "GarageCond",
               "Alley",
               "Fence",
               "FireplaceQu",
               "MiscFeature",
               "PoolQC")

all_dat <- 
  all_dat %>% 
  mutate_at(vars(all_of(none_vars)), ~ replace_na(., "None"))

zero_vars <- c("MasVnrArea",
               "BsmtFinSF1",
               "BsmtFinSF2",
               "BsmtFullBath",
               "BsmtHalfBath",
               "BsmtUnfSF",
               "TotalBsmtSF",
               "GarageArea",
               "GarageCars",
               "GarageYrBlt")

all_dat <- 
  all_dat %>% 
  mutate_at(vars(all_of(zero_vars)), ~ replace_na(., 0))
```

In Python, I will do the same thing using `for` loops for the relevant features, taking advantage of `{pandas}` `fillna()` method.
```{python}
all_dat = pd.concat((train, test), sort = False, ignore_index = True)
all_dat.drop(['Id', 'SalePrice'], axis = 1, inplace = True)

# None vars
none_vars = ['BsmtCond',
             'BsmtQual',
             'BsmtExposure',
             'BsmtFinType1',
             'BsmtFinType2',
             'MasVnrType',
             'GarageType',
             'GarageFinish',
             'GarageQual',
             'GarageCond',
             'Alley',
             'Fence',
             'FireplaceQu',
             'MiscFeature',
             'PoolQC']

for var in none_vars:
    all_dat[var] = all_dat[var].fillna('None')
    
# Zero vars
zero_vars = ['MasVnrArea',
             'BsmtFinSF1',
             'BsmtFinSF2',
             'BsmtFullBath',
             'BsmtHalfBath',
             'BsmtUnfSF',
             'TotalBsmtSF',
             'GarageArea',
             'GarageCars',
             'GarageYrBlt']

for var in zero_vars:
    all_dat[var] = all_dat[var].fillna(0)
```

Moving on to the observations missing at random. Most of these are categorical, and so I impute the mode. For the numeric feature--`LotFrontage`--I impute the median. Don't forget to `ungroup()`!
```{r}
random_vars <- c("Electrical", "Exterior1st", "Exterior2nd", "Functional", "KitchenQual", "MSZoning", "SaleType", "Utilities")

all_dat <- 
  all_dat %>% 
  group_by(Neighborhood, MSSubClass) %>% 
  mutate_at(vars(all_of(random_vars)), ~ ifelse(is.na(.), which.max(table(.)) %>% names(), .)) %>% 
  group_by(Neighborhood) %>% 
  mutate(LotFrontage = ifelse(is.na(LotFrontage), median(LotFrontage, na.rm = TRUE), LotFrontage)) %>% 
  ungroup()
```

In Python, the approach is similar, using the `groupby()` method to group by neighborhood and class, along with the `fillna()` method. This approach uses a `lambda` function within a for loop. This is like using an anonymous function within an `apply` loop in R.
```{python}
random_vars = ['Electrical', 'Exterior1st', 'Exterior2nd', 'Functional', 'KitchenQual', 'MSZoning', 'SaleType', 'Utilities']

for var in random_vars:
    all_dat[var] = all_dat.groupby(['Neighborhood', 'MSSubClass'])[var].apply(lambda x: x.fillna(x.mode()[0]))
    
all_dat['LotFrontage'] = all_dat.groupby(['Neighborhood'])['LotFrontage'].apply(lambda x: x.fillna(x.median()))
```

Now, we should have NO missing values in our data!
```{r}
glue("There are {sum(is.na(all_dat))} features with missing observations")
```

```{python}
missing = all_dat.apply(lambda x: x.isnull().sum()) > 0
print('There are {} features with missing observations'.format(missing.sum()))
```

# Feature engineering

```{r}
all_dat <-
  all_dat %>% 
  mutate(TotalArea = `1stFlrSF` + `2ndFlrSF` + TotalBsmtSF,
         AvgRmSF = GrLivArea / TotRmsAbvGrd,
         TotalBaths = FullBath + (HalfBath * 0.5) + BsmtFullBath + (BsmtHalfBath * 0.5),
         OutsideArea = OpenPorchSF + `3SsnPorch` + EnclosedPorch + ScreenPorch + WoodDeckSF
         #OverallQual_x_OverallCond = OverallQual * OverallCond,
         #YearBuilt_x_OverallQual = YearBuilt * OverallQual,
         #YearBuilt_x_OverallCond = YearBuilt * OverallCond) 
         )%>% 
  mutate(TotalArea2 = TotalArea^2,
         OverallQual2 = OverallQual^2)
```

```{python}
# Feature engineering
all_dat['TotalArea'] = all_dat['1stFlrSF'] + all_dat['2ndFlrSF'] + all_dat['TotalBsmtSF']
all_dat['AvgRmSF'] = all_dat['GrLivArea'] / all_dat['TotRmsAbvGrd']
all_dat['TotalBaths'] = all_dat['FullBath'] + (all_dat['HalfBath'] * 0.5) + all_dat['BsmtFullBath'] + (all_dat['BsmtHalfBath'] * 0.5)
all_dat['OutsideArea'] = all_dat['OpenPorchSF'] + all_dat['3SsnPorch'] + all_dat['EnclosedPorch'] + all_dat['ScreenPorch'] + all_dat['WoodDeckSF']
#all_dat['OverallQual_x_OverallCond'] = all_dat['OverallQual'] * all_dat['OverallCond']
#all_dat['YearBuilt_x_OverallQual'] = all_dat['OverallQual'] * all_dat['YearBuilt']
#all_dat['YearBuilt_x_OverallCond'] = all_dat['OverallCond'] * all_dat['YearBuilt']
all_dat['TotalArea2'] = all_dat['TotalArea']**2
all_dat['OverallQual2'] = all_dat['OverallQual']**2

```


```{r}
qual_vars = c("BsmtCond", "BsmtQual", "ExterCond", 
             "ExterQual", "FireplaceQu", "GarageCond", 
             "GarageQual", "HeatingQC", "KitchenQual", "PoolQC")
all_dat <-
  all_dat %>% 
  mutate_at(vars(all_of(qual_vars)), ~ factor(., 
                                              levels = c("None",
                                                         "Po",
                                                         "Fa",
                                                         "TA",
                                                         "Gd",
                                                         "Ex"), 
                                              ordered = TRUE)) %>% 
  mutate(Functional = factor(Functional, 
                             levels = c("Sev",
                                        "Maj2",
                                        "Maj1",
                                        "Mod",
                                        "Min1",
                                        "Min2",
                                        "Typ"),
                             ordered = TRUE),
         GarageFinish = factor(GarageFinish,
                               levels = c("None",
                                          "Unf",
                                          "RFn",
                                          "Fin"),
                               ordered = TRUE),
         BsmtFinType1 = factor(BsmtFinType1,
                               levels = c("None",
                                          "Unf",
                                          "LwQ",
                                          "Rec",
                                          "BLQ",
                                          "ALQ",
                                          "GLQ"),
                               ordered = TRUE),
         BsmtFinType2 = factor(BsmtFinType2,
                               levels = c("None",
                                          "Unf",
                                          "LwQ",
                                          "Rec",
                                          "BLQ",
                                          "ALQ",
                                          "GLQ"),
                               ordered = TRUE))
```


```{python}
qual = {'None': 0, 'Po': 1, 'Fa': 2, 'TA': 3, 'Gd': 4, 'Ex': 5}
func = {'Sev': 0, 'Maj1': 1, 'Maj2': 1, 'Mod': 2, 'Min1': 3, 'Min2': 3, 'Typ': 4}
fin = {'None': 0, 'Unf': 1, 'RFn': 2, 'Fin': 3}
bsmt_fin = {'None': 0, 'Unf': 1, 'LwQ': 2, 'Rec': 3, 'BLQ': 4, 'ALQ': 5, 'GLQ': 6}

qual_vars = ['BsmtCond', 'BsmtQual', 'ExterCond', 
             'ExterQual', 'FireplaceQu', 'GarageCond', 
             'GarageQual', 'HeatingQC', 'KitchenQual', 'PoolQC']

for var in qual_vars:
    all_dat[var] = all_dat[var].map(qual)
    
all_dat['Functional'] = all_dat['Functional'].map(func)
all_dat['GarageFinish'] = all_dat['GarageFinish'].map(fin)
all_dat['BsmtFinType1'] = all_dat['BsmtFinType1'].map(bsmt_fin)
all_dat['BsmtFinType2'] = all_dat['BsmtFinType2'].map(bsmt_fin)
```

There are some _numeric_ features that are really categorical, so we should encode these as well. However, they are not ordinal and so we can convert them to strings. These include year, month, and subclass.
```{r}
nominal_vars = c("Alley", "BldgType", "Condition1", "Condition2", "Electrical", 
                "Exterior1st", "Exterior2nd", "Fence", "Foundation", "GarageType", 
                "Heating", "HouseStyle", "LandContour", "LotConfig", "MSSubClass",
                "MSZoning", "MasVnrType", "MiscFeature", "MoSold", "Neighborhood",
                "RoofMatl", "RoofStyle", "SaleCondition", "SaleType", "YrSold",
                "CentralAir", "BsmtExposure", "LandSlope", "LotShape", "PavedDrive",
                "YearBuilt", "YearRemodAdd", "Street", "Utilities")

all_dat <-
  all_dat %>% 
  mutate_at(vars(all_of(nominal_vars)), as.character)
```

```{python}
nominal_vars = ['Alley', 'BldgType', 'Condition1', 'Condition2', 'Electrical', 
                'Exterior1st', 'Exterior2nd', 'Fence', 'Foundation', 'GarageType', 
                'Heating', 'HouseStyle', 'LandContour', 'LotConfig', 'MSSubClass',
                'MSZoning', 'MasVnrType', 'MiscFeature', 'MoSold', 'Neighborhood',
                'RoofMatl', 'RoofStyle', 'SaleCondition', 'SaleType', 'YrSold',
                'CentralAir', 'BsmtExposure', 'LandSlope', 'LotShape', 'PavedDrive',
                'YearBuilt', 'YearRemodAdd', 'Street', 'Utilities']

for var in nominal_vars:
    all_dat[var] = all_dat[var].astype(str)
```


I will use one-hot encoding to turn nominal features into dummies. In Python, `{pandas}` has a built-in method to accomplish this. In R, one can use `getDummies()` from the `{caret}` package--which is superseded a bit by the `step_dummy()` function in `{recipes}`. But I'll use the base R approach to avoid adding any additional packages into the mix.
```{r}
dummies <- model.matrix(~.-1, data = all_dat %>% select_if(is.character))

all_dat <-
  all_dat %>% 
  select_if(negate(is.character)) %>% 
  bind_cols(dummies %>% as_tibble())
```

Quick and easy in Python.
```{python}
dummies = pd.get_dummies(all_dat[nominal_vars])
all_dat = pd.concat((all_dat.drop(nominal_vars, axis = 1), dummies), sort = False, axis = 1)
```

As our last step, transform the variables using the Box-Cox method with a fudge factor of `+1`. I want to get the `lambda` for each feature so as to normalize that feature accurately. So first, I determine which features have a skewed distribution that I want to transform. I get the `lambda` for those features and then transform them using that `lambda`. I drop the skewed columns from the data frame and then bind the transformed features.
```{r}
skewed_vars <- 
  map_lgl(all_dat %>% select_if(is.numeric), ~ (moments::skewness(.x) > 0.75)) %>% which() %>% names()

lmbdas <- map_dbl(skewed_vars, ~ car::powerTransform(all_dat[[.x]] + 1)$lambda)

trans_vars <- map2_dfc(skewed_vars, lmbdas, ~ car::bcPower(all_dat[[.x]] + 1, lambda = .y))

colnames(trans_vars) <- skewed_vars

all_dat <-
  all_dat %>% 
  select(-all_of(skewed_vars)) %>% 
  bind_cols(trans_vars)
```

In Python, the `boxcox` function from the `scipy.stats` module automatically calculates `lambda` if it is not specified.
```{python}
from scipy.stats import boxcox

numeric_feats = all_dat.dtypes[all_dat.dtypes != "object"].index

skewed_cols = all_dat[numeric_feats].apply(lambda x: skew(x.dropna()))
skewed_cols_idx = skewed_cols[skewed_cols > 0.75].index
skewed_cols = all_dat[skewed_cols_idx]+1
all_dat[skewed_cols_idx] = skewed_cols.transform(lambda x: boxcox(x)[0])
```

The R and Python approaches produce the same values for `lambda`. Just double-checking.

R: 
```{r}
glue("Lambda value for LowQualFinSF in R is: {car::powerTransform(train[['LowQualFinSF']] + 1)$lambda}")
```

Python:
```{python}
xd, lam = boxcox((train['LowQualFinSF'])+1)
print('Lambda value for LowQualFinSF in Python is: %f' % lam)
```


Double-check that there are no missing observations about all this feature engineering.
```{r}
glue("There are {sum(is.na(all_dat))} features with missing observatoins")
```

```{python}
missing = all_dat.apply(lambda x: x.isnull().sum()) > 0
print('There are {} features with missing observations'.format(missing.sum()))
```


Great, ready to start modeling!

# Modeling

The first thing I will do is split the training data into another pair of training and testing. I will fit each model on the training data and evaluate it on the testing data. I need to subset the `all_dat` data frame to include only the values in the initial training data set and then split into a training and testing set using a 70/30 split. There are many ways to do this here by hand, but there are also a few packages that have functions to accomplish this. I'm particularly fond of [`initial_split()`](https://tidymodels.github.io/rsample/reference/initial_split.html) from the `{rsample}` package that is part of the `{tidymodels}` ecosystem. I'm going to use the `{tidyverse}` ecosystem to tune and model, so I'll using `{rsample}` here. The `initial_split()` function creates a training and testing set that are called using `training()` and `testing()`. I will further use cross-validation to for model selection. The `vfold_cv()` function creates an n-fold cross-validation split in the data set. Here, we use it on the training split of the training data.
```{r}
train_dat <- all_dat %>% slice(1:nrow(train)) %>% janitor::clean_names()

# Add the log of the target back to the training data for modeling
train_dat[["SalePrice"]] <- log(train[["SalePrice"]])

#sz <- ceiling(nrow(X)*.7)
#
#set.seed(1491)
#train_idx <- sort(sample(nrow(X), sz))
#X_train <- X[train_idx, ]
#X_test <- X[-train_idx, ]
#
#y_train <- y[train_idx]
#y_test <- y[-train_idx]

set.seed(1491)
train_split <- initial_split(train_dat, prob = 0.7)
train_cv <- training(train_split) %>% vfold_cv(v = 5)

glue("The training data is {nrow(training(train_split))} rows by {ncol(training(train_split))} columns")
glue("The testing data is {nrow(testing(train_split))} rows by {ncol(testing(train_split))} columns")
```

For Python, after I subset the `all_dat` data from to include only the data in the initial training dataset, I use the `train_test_split()` function from sci-kit learn.
```{python}
from sklearn.model_selection import train_test_split

X = all_dat.iloc[:len(train),:]
y = np.log(train['SalePrice'])
test = all_dat.iloc[len(train):,:]

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state = 1491)

print('The training data is {} rows by {} columns'.format(X_train.shape[0], X_train.shape[1]))
print('The testing data is {} rows by {} columns'.format(X_test.shape[0], X_test.shape[1]))
```

## Regularized regression

To tune and fit the various models I will try in R, I will use a set of packages from the [`{tidymodels}`](https://github.com/tidymodels) ecosystem. These packages were developed, in part, by the creator of the`{caret}` package and designed to supplant it. To break it down: I will use `{parsnip}` to define and fit models, `{dials}` and `{tune}` to tune the hyperparameters, and `{rsample}` to cross-validate, and `{yardstick}` to evaluate the model.
```{r}
lasso_model <-
    linear_reg(
        penalty = tune(),
        mixture = 1) %>%
    set_engine("glmnet",
               standardize = TRUE)

alphas <- grid_regular(penalty(range = c(-5, 0)), levels = 25)
alphas_py <- alphas %>% pull(penalty)

lasso_cv <-
    tune_grid(
        formula = SalePrice ~ .,
        model = lasso_model,
        resamples = train_cv,
        grid = alphas,
        metrics = metric_set(rmse),
        control = control_grid(verbose = FALSE)
    )

best_lasso <-
    lasso_cv %>%
    select_best("rmse", maximize = FALSE)

print(best_lasso)

best_lasso_model <-
    lasso_model %>%
    finalize_model(parameters = best_lasso)

lasso_fit <-
    best_lasso_model %>%
    fit(SalePrice ~ ., training(train_split))

lasso_predictions <- predict(lasso_fit, testing(train_split))

testing(train_split) %>%
  select(SalePrice) %>%
  bind_cols(lasso_predictions) %>%
  rmse(SalePrice, .pred) %>% 
  pull(.estimate) %>% 
  {glue("For the LASSO model in R, the RMSE is {round(., 4)}")}
```



Now, in Python, using the same hyperparameters. 
```{python}
from sklearn.metrics import mean_squared_error
from sklearn.linear_model import LassoCV, RidgeCV, ElasticNetCV
from sklearn.model_selection import KFold, GridSearchCV, cross_val_score

K = 5
kf = KFold(n_splits=K, shuffle=True, random_state=1491)

def rmse(y_train, y_pred):
    return np.sqrt(mean_squared_error(y_train, y_pred))

alphas = r.alphas_py

lasso = LassoCV(max_iter=1e7, 
                alphas=alphas, 
                random_state=1491, 
                cv=5,
                n_jobs=-1,
                normalize=True)

lasso.fit(X_train, y_train)
lasso_pred = lasso.predict(X_test)
print("For the LASSO model in Python, the RMSE is {}".format(rmse(y_test, lasso_pred).round(4)))
```


## Ridge model
```{r}
ridge_model <-
    linear_reg(
        penalty = tune(),
        mixture = 0) %>%
    set_engine("glmnet",
               standardize = TRUE)

alphas <- grid_regular(penalty(range = c(-5, 0)), levels = 25)
alphas_py <- alphas %>% pull(penalty)

ridge_cv <-
    tune_grid(
        formula = SalePrice ~ .,
        model = ridge_model,
        resamples = train_cv,
        grid = alphas,
        metrics = metric_set(rmse),
        control = control_grid(verbose = FALSE)
    )

best_ridge <-
    ridge_cv %>%
    select_best("rmse", maximize = FALSE)

print(best_ridge)

best_ridge_model <-
    ridge_model %>%
    finalize_model(parameters = best_ridge)

ridge_fit <-
    best_ridge_model %>%
    fit(SalePrice ~ ., training(train_split))

ridge_predictions <- predict(ridge_fit, testing(train_split))

testing(train_split) %>%
  select(SalePrice) %>%
  bind_cols(ridge_predictions) %>%
  rmse(SalePrice, .pred) %>% 
  pull(.estimate) %>% 
  {glue("For the ridge model in R, the RMSE is {round(., 4)}")}
```

```{python}
from sklearn.linear_model import RidgeCV
alphas = r.alphas_py

ridge = RidgeCV(alphas=alphas, 
                cv=5,
                normalize=True)

ridge.fit(X_train, y_train)
ridge_pred = ridge.predict(X_test)
print("For the Ridge model in Python, the RMSE is {}".format(rmse(y_test, ridge_pred).round(4)))
```

## Elastic net model
```{r}
en_model <-
    linear_reg(
        penalty = tune(),
        mixture = tune()) %>%
    set_engine("glmnet",
               standardize = TRUE)

en_grid <- grid_regular(penalty(range = c(-5, 0)), mixture(), levels = 5)
alphas_py <- en_grid %>% pull(penalty) %>% unique() %>% round(5)
l1 <- en_grid %>% pull(mixture) %>% unique()  

en_cv <-
    tune_grid(
        formula = SalePrice ~ .,
        model = en_model,
        resamples = train_cv,
        grid = en_grid,
        metrics = metric_set(rmse),
        control = control_grid(verbose = FALSE)
    )

best_en <-
    en_cv %>%
    select_best("rmse", maximize = FALSE)

print(best_en)

best_en_model <-
    en_model %>%
    finalize_model(parameters = best_en)

en_fit <-
    best_en_model %>%
    fit(SalePrice ~ ., training(train_split))

en_predictions <- predict(en_fit, testing(train_split))

testing(train_split) %>%
  select(SalePrice) %>%
  bind_cols(en_predictions) %>%
  rmse(SalePrice, .pred) %>% 
  pull(.estimate) %>% 
  {glue("For the Elastic Net model in R, the RMSE is {round(., 4)}")}
```

```{python}
#alphas = r.alphas_py
#l1 = r.l1
alphas = [0.00001, 0.00018, 0.00316, 0.05623, 1.00000]
l1 = [0.8, 0.85, 0.9, 0.95, 0.99, 1]

elasticnet = ElasticNetCV(max_iter=1e7, 
                          alphas=alphas, 
                          l1_ratio=l1,
                          cv=5)
                                                        
elasticnet.fit(X_train, y_train)
elasticnet_pred = elasticnet.predict(X_test)
print("For the Elastic Net model in Python, the RMSE is {}".format(rmse(y_test, elasticnet_pred).round(4)))
```



## XGBoost
```{r}

# R Parsnip to Python translator:
# sample_size = subsample
# loss_reduction = gamma
# trees = n_estimators
# tree_depth = max_depth
# min_n = min_child_weight
# mtry = colsample_bytree

#xgb_model <-
#  boost_tree(
#    mode = "regression",
#    trees = tune(),
#    learn_rate = tune(),
#    min_n = tune(),
#    tree_depth = tune()
#  ) %>%
#  set_engine(
#    "xgboost",
#    booster = "gbtree"
#  )
#
#xgb_grid <-
#    grid_regular(
#        trees(c(100, 1000)),
#        learn_rate(c(-4, 0)),
#        min_n(c(2, 10)),
#        tree_depth(c(2, 8)),
#        levels = 3
#    )
#
#learn_rate_py <- xgb_grid %>% pull(learn_rate) %>% unique()
#min_n_py <- xgb_grid %>% pull(min_n) %>% unique()
#tree_depth_py <- xgb_grid %>% pull(tree_depth) %>% unique()
#
#xgb_tune <-
#  tune_grid(
#    formula   = SalePrice ~ .,
#    model     = xgb_model,
#    resamples = train_cv,
#    grid      = xgb_grid,
#    metrics   = metric_set(rmse),
#    control   = control_grid(verbose = TRUE)
#  )
#
#best_xgb <-
#  xgb_tune %>%
#  select_best("rmse", maximize = FALSE)
#
#print(best_xgb)
#
#xgb_fit <-
#  xgb_model %>%
#  finalize_model(parameters = best_xgb) %>%
#  fit(SalePrice ~ ., training(train_split))

xgb_model <-
  boost_tree(
    mode = "regression",
    trees = 2000,
    sample_size = 0.5,
    mtry = 1,
    loss_reduction = 0.001,
    learn_rate = 0.01,
    min_n = 10,
    tree_depth = 5
  ) %>%
  set_engine(
    "xgboost",
    booster = "gbtree"
  )

xgb_fit <-
  xgb_model %>%
  fit(SalePrice ~ ., training(train_split))

xgb_predictions <-
  testing(train_split) %>%
  select(SalePrice) %>%
  bind_cols(
    predict(xgb_fit, testing(train_split))
  )

testing(train_split) %>%
  select(SalePrice) %>%
  bind_cols(xgb_predictions) %>%
  rmse(SalePrice, .pred) %>% 
  pull(.estimate) %>% 
  {glue("For the XGBoost model in R, the RMSE is {round(., 4)}")}
```



```{python}
from xgboost import XGBRegressor

xgb = XGBRegressor(booster='gbtree',
                   objective='reg:squarederror',
                   eta = 0.01,
                   subsample=0.5,
                   gamma = 0,
                   n_estimators=2000,
                   colsample_bytree=1,
                   max_depth=5, 
                   min_child_weight=10, 
                   random_state=1491)

xgb_model = xgb.fit(X_train, y_train)
xgb_pred = xgb_model.predict(X_test)
print("For the XGBoost model in Python, the RMSE is {}".format(rmse(y_test, xgb_pred).round(4)))
```

```{r}
mars_model <-
  mars(
    mode = "regression",
    prod_degree = tune(),
    prune_method = tune()
  ) %>%
  set_engine("earth")

mars_grid <-
    grid_regular(
        prod_degree(),
        prune_method(c('backward', 'none', 'exhaustive', 'forward', 'seqrep')),
        levels = 5
    )

mars_tune <-
  tune_grid(
    formula   = SalePrice ~ .,
    model     = mars_model,
    resamples = train_cv,
    grid      = mars_grid,
    metrics   = metric_set(rmse),
    control   = control_grid(verbose = FALSE)
  )

best_mars <-
  mars_tune %>%
  select_best("rmse", maximize = FALSE)

print(best_mars)

mars_fit <-
  mars_model %>%
  finalize_model(parameters = best_mars) %>%
  fit(SalePrice ~ ., training(train_split))

mars_predictions <-
  testing(train_split) %>%
  select(SalePrice) %>%
  bind_cols(
    predict(mars_fit, testing(train_split))
  )

testing(train_split) %>%
  select(SalePrice) %>%
  bind_cols(mars_predictions) %>%
  rmse(SalePrice, .pred) %>% 
  pull(.estimate) %>% 
  {glue("For the MARS model in R, the RMSE is {round(., 4)}")}
```

```{python}
from pyearth import Earth

mars_model = Earth(max_degree = 1, enable_pruning=False)
mars_model.fit(X_train, y_train)

mars_pred = mars_model.predict(X_test)
print("For the MARS model in Python, the RMSE is {}".format(rmse(y_test, mars_pred).round(4)))
```

# Averaging models for the final predication
```{r}
preds <- list(lasso_predictions, ridge_predictions, en_predictions, xgb_predictions, mars_predictions)
names(preds) <- c("LASSO", "Ridge", "Elastic Net", "XGBoost", "MARS")

imap_chr(preds, ~ testing(train_split) %>% 
             select(SalePrice) %>%
             bind_cols(.x) %>%
             rmse(SalePrice, .pred) %>% 
             pull(.estimate) %>% 
             {glue("For the {.y} model in R, the RMSE is {round(., 4)}")})

all_pred <- imap(preds, ~ .x %>% pull(.pred))

weighted_pred <- (all_pred$LASSO * 0.2) + (all_pred$Ridge * 0.1) + (all_pred$`Elastic Net` * 0.2) + (all_pred$XGBoost * 0.1) + (all_pred$MARS * 0.4)

rmse_vec(testing(train_split)[["SalePrice"]], weighted_pred) %>% 
    {glue("For the averaged model, the RMSE is {round(., 4)}")}
```

```{python}
for name, model in zip(['Lasso', 'Ridge', 'Elastic Net', 'XGBosst', 'MARS'], [lasso, ridge, elasticnet, xgb_gs, mars_model]):
  print("For the {} model in Python, the RMSE is {}".format(name, rmse(y_test, model.predict(X_test)).round(4)))

blended_pred = ((0.2 * lasso.predict(X_test)) + (0.1 * ridge.predict(X_test)) + (0.3 * elasticnet.predict(X_test)) + (0.1 * xgb_gs.predict(X_test)) + (0.3 * mars_model.predict(X_test)))
            
print("For the averaged model, the RMSE is {}".format(rmse(y_test, blended_pred).round(4)))
```
